
%% MAB_for_task_allocation.tex
%% V1.4a
%% 2016/04/15
%% by lemondy
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8a or later) with an IEEE
%% Computer Society journal paper.
%%

\documentclass[10pt,journal,compsoc]{IEEEtran}
%

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{epsfig}
\usepackage{latexsym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[noend]{algorithmic}
\usepackage[ruled]{algorithm}
\usepackage{paralist}
\usepackage{caption}
\usepackage{multirow}

\newtheorem{Lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{Theorem}{Theorem}

%缩进
\usepackage{indentfirst}
%
\usepackage{booktabs}

\newtheorem{myDef}{Definition}
\newtheorem{myTheo}{Theorem}
%中文
\usepackage[no-math]{xeCJK}
\setCJKmainfont[BoldFont=SimHei, ItalicFont=KaiTi]{SimSun}
\setCJKmonofont{KaiTi}


% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi




% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi



% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{A Multiarmed Bandit Incentive Mechanism for\\ Task Allocation in Crowdsensing}
%

\author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
        John~Doe,~\IEEEmembership{Fellow,~OSA,}
        and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem M. Shell is with the Department
of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
GA, 30332.\protect\\
% note need leading \protect in front of \\ to get a newline within \thanks as
% \\ is fragile and will error, could use \hfil\break instead.
E-mail: see http://www.michaelshell.org/contact.html
\IEEEcompsocthanksitem J. Doe and J. Doe are with Anonymous University.}% <-this % stops an unwanted space
\thanks{Manuscript received April 19, 2005; revised September 17, 2014.}}



% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~13, No.~9, September~2014}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}

\IEEEtitleabstractindextext{%
\begin{abstract}
The multi-armed bandit problem has attracted remarkable attention in the machine learning community and many efficient algorithms have been proposed to handle the so-called exploitation-exploration dilemma in various bandit setups. At the same time, significantly less effort has been devoted to adapting bandit algorithms to particular architectures, such as sensor networks, multi-core machines, or peer-to-peer (P2P) environments, which could potentially speed up their convergence.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Task Allocation, multiarmed bandits, cellular network, reward maximization, Thompson sampling
\end{IEEEkeywords}}


% make the title area
\maketitle

\IEEEdisplaynontitleabstractindextext
% \IEEEdisplaynontitleabstractindextext has no effect when using
% compsoc or transmag under a non-conference mode.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}


Mobile crowdsensing is a new type of collaboration which involves a crowd of mobile users who uses their carried smart phone to execute some complex computation or sensing services in some environment. Since it can utilize the mobility of users to solve large-scale mobile sensing tasks, it has stimulated a number of attractive applications, such as urban WiFi characterization, traffic information mapping, and so on. Moreover, there have been some platforms, frameworks, and incentive mechanism, designed for crowdsensing in all kinds of scenario.\cite{xiao2015multi}\cite{jain2014multiarmed}

Consider that a user carried mobile phone in an mobile social network has some independent sensing tasks, such as sample WiFi fingerprint, traffic information mapping, and so on. However, these tasks cost too much time and exceed its processing ability. Then, it requests other mobile users for help by starting a crowdsensing. In this scenario, the requester will send tasks to and receive results from other mobile users, of which can by calling other or by send message to communicate. And it often involves large size data transmissions, however, we can incent many users joining it, such that can decrease every user's work time. \cite{xiao2015multi} adapts short-distance wireless communication technologies (e.g., WiFi or Bluetooth), in contrast, we consider a different communication technology, which through cellular network get in touch with each other. In this situation, the requester doesn't have to stay not far from the worker, and what's more, the requester can do another work after publishes tasks. Fig.1 depicts a simple example of a task allocation procedure in crowdsensing. At some time, the requester wants to collect some data, and then it send message to the work ask they bid for the task. After the work return their bid and workload, then the requester make a decision to choose a best worker to handle these tasks. The best worker means he will finish the worker very well and at the same time the bid is reasonable. Then, an important problem is how the requester allocates these task to workers, so as to for every task can choose a best worker to finish it.

%\begin{figure}[!t]
%  \centerline{
%  \begin{tabular}{cc}
%  \epsfysize=4in\epsfbox{./fig/mobile_social_network.eps}
%  \end{tabular}}
%  \caption{Results on random networks ($n=20$, $m=10$). }
%  \label{fig:k_throwboxes_random_20}
%\end{figure}
\begin{figure}[H]
\begin{center}
  \includegraphics[width=0.4\textwidth]{./fig/mobile_crowdsensing.eps}
  \caption{mobile social network}\label{MSN}
\end{center}
\end{figure}

In this paper, we focus on the above task allocation problem. In this problem, choosing a best worker from the requester's contacts. Selects best worker just like choose excellent contractor, which will let the utility maximize of the requester.

To solve the above task allocation problem in mobile crowdsensing, we proposed a based multi-armed bandits algorithm with incentive mechanism, which is called Thompson sampling task allocation(TS-TA). We choose Thompson sampling to handle this problem.

Thompson sampling(TS) is one of the earliest heuristics for multi-armed bandit problems. The first Bayesian heuristics is around 80 years old, dating to Thompson(1933). Since then, it has been rediscovered numerous times independently in the context of reinforcement learning, e.g., in [?][?]. It is a member of the family of \textit{randomized probability matching} algorithms. The basic idea is to assume a simple prior distribution on the underlying parameters of the reward distribution of every arm, and at every time step, play an arm according to its posterior probability of being the best arm. The general structure of TS for the contextual bandits problem involves the following elements:

 \begin{itemize}
   \item a set $\theta $ of parameters ${\tilde \mu }$ ;
   \item a prior distribution $P({\tilde \mu})$ on these parameters;
   \item past observations $D$ consisting of (context b, reward r) for the past time steps
   \item a likelihood function $P(r|b,\tilde \mu )$, which gives the probability of reward given a context b and a parameter ${\tilde \mu}$;
   \item a posterior distribution $P(\tilde \mu |D) \propto P(D|\tilde \mu )P(\tilde \mu )$, where $P(D|\tilde \mu)$  is the likelihood function.
 \end{itemize}

 In each round, TS plays an arm according to its posterior probability of having the best parameter. A simple way to achieve this is to produce a sample of parameter for each arm, using the posterior distribution, and play the arm that produces the best sample.

\section{RELATED WORK}

\subsection{Task allocation}

\subsection{Single-Agent Multi-Armed Bandit}

Single-agent multi-armed bandit (SA-MAB) problem was first introduced in [1]. In the most seminal setting, the problem models an agent facing the challenge of sequentially selecting an arm from a set of arms in order to receive an a priori unknown reward, drawn from some unknown reward generating process. As a result of lack of prior information, at each trial, the player may choose some inferior arm in terms of reward, yielding some regret that is quantified by the difference between the reward that would have been achieved had the player selected the best arm and the actual achieved reward. In such an unknown setting, the player decides which arm to pull in a sequence of trials so that its accumulated regret over the game horizon is minimized. This problem is an instance of exploration-exploitation dilemma, i.e., the tradeoff between taking actions that yield immediate large rewards on the one hand and taking actions that might result in larger reward only in future, for instance activating an inferior arm only to acquire information, on the other hand. Therefore, SAMAB is mainly concerned with a sequential online decision making problem in an unknown environment. A solution of a bandit problem is thus a decision making strategy called policy or allocation rule, which determines which arm should be played at successive rounds. Policies are in general evaluated based on their regret or discounted reward performance.

As mentioned before, MAB benefits from a wide range of variations in the setting. In the event that arms have different states, the reward depends on arms' states, and the bandit model is referred to as stateful (Markovian). Otherwise, the model is stateless, which itself can be divided to few subsets. As stateful and stateless bandits are inherently different, we discuss them separately in the following.





\begin{table}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}

\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{|c|c|}
\hline
\hline
Multi-armed Bandit(MAB) & Task Allocation(TA)\\
\hline
\hline
Player & Requester\\

Armed bandit & Workers\\

Pull level & Select worker\\
\hline
\hline
\end{tabular}
\caption{Parallel between Multi-armed bandit and Task Allocation}
\end{table}


\begin{table}[!t]
%% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\centering
\begin{tabular}{|c|c|}
\hline
\hline
Notation & Description\\
\hline
\hline
K & Set of all workers in the requester's contacts\\

m & Number of tasks\\

$b _i(t)$ & Context of arm i at time t\\

$\mu$ & The prior parameter of the arm\\

$r _i(t)$ & The reward of arm i at time t\\

$\pi$ & The allocation mechanism\\

$RE ^ \pi$ & The total regret\\

${p _i}^t$ & Payment of arm i\\
\hline
\hline
\end{tabular}
\caption{Notation used}
\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively.
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.
\section{A Parallel with Crowdsourcing}
"Crowdsourcing is the act of taking job traditionally performed by a designated agent(usually an employee) and outsourcing it to an undefined, generally large group of people in the form of an open call", this is what Jeff Howe ever offered the definition of crowdsourcing. The designated agent is often called the requester, while people working on the task are called crowd workers. The requester needs to assign the task to these workers with unknown capability and willingness to complete the task based on the goals to achieve. In mobile crowdsensing network, there is a task publisher at some time, and he/she can chooses a worker from his/her contacts. The task publisher just like the requester, while his/her contacts just like the group of works. Table 1 precisely demonstrates this paralle between crowdsoucing and mobile crowdsensing.

\section{A Parallel with MAB}


\section{Related Work on MAB Problem}


\section{Model\&Problem}
%Transform the task allocation into a multi-armed bandit problem.

\subsection{Model for Task Allocation}
Consider a network denoted by a graph ${G^t} = ({V^t},{E^t})$ at time t with a set ${V^t} = \{ {v_i}|i = 1,...,K\}$ of K nodes represented the node set who meet the task requester node represented by ${v_0}$ at time t, a set E of edges denoting ${v_0}$ can communicate with other nodes. And the graph is star network structure. The system is been divided into time slot, and at each round t, node ${v_0}$ has K choices of relay node, where relay nodes are i.i.d. Suppose the ability of every node accomplish a task as stochastic process ${\chi _i}(t)$ over time with a mean ${\mu _i} \in [0,1]$, as done in many previous works [1] [2] [3].

\setlength{\parindent}{2em}At each round t, an K-dimensional \emph{strategy} vector ${\Phi _0}(t) = \{ {\Phi _{0,i}}|i = 1,...,K\}$ is selected under some \emph{policy} from the \emph{feasible strategy set} F. Here ${\Phi _{0,i}}$ is the index of relay node selected by node carried message in strategy $\Phi _0$. We use i=1,...,K to index strategies of feasible set F in XXXXX. By feasible we mean at that time slot, carried message node meet another k nodes(who can communicate with the message carrier directly). When a strategy $\Phi _0$ is determined, then one node $v _i$ will be selected, and it will give back a reward $r _i(t)$. And then the total reward of T round is defined as $R = \sum\nolimits_{t = 1}^T {r(t)}$. The goal of this allocation problem is to maximize the accumulated reward $R$ over the T rounds. Obviously, if the reward of every node is constant, then we can simply choose the node who will have maximize reward every time slot. While in our problem, the reward is variable over time. And it's probability distribution follows $f(x,\theta )$ where $\theta$ is an unknown parameter. The gap between the actual total reward we get and the optimal total reward theoretically is call regret of the algorithm, and it can be formally written as $RE = \sum\nolimits_{t = 1}^T {{r^*}(t)}  - \sum\nolimits_{t = 1}^T {r(t)}$, where $r^*$ is the max reward we can get every time slot. The goal of the algorithm is to keep the regret $RE$ as low as possible.

\setlength{\parindent}{2em}An allocation strategy(also called a policy) $\pi$ specifies the action chosen by the task requester(carried message node) at each time in the system. Formally, $\pi$ is a sequence of random variables $\pi  = \{ {\pi ^1},{\pi ^2},...,{\pi ^T}\}$ where ${\pi ^t} = \{ a_1^t,a_2^t,...,a_k^t\}$ is the vector of zeros and ones(deterministic mechanism). If $a_i^t=1$ then node i will be chosen as a relay node at slot t, whereas, $a_i^t=0$ means node i doesn't be chosen at all, i.e., $\forall t,\sum\nolimits_i {{a^t}}  \le 1$.

%%考虑arm的payoff是线性的。
There are K arms at time t for a requester, a context vector ${b_i}(t) \in {R^d}$, is revealed for every arm i. These context vectors are chosen by a requester in an adaptive manner after observing the arms played and their rewards up to time t-1, i.e. history $H_{t-1}$,

${H_{t - 1}} = \{ a(\tau ),{r_{a(\tau )}}(\tau ),{b_i}(\tau ),i = 1,...,K,\tau  = 1,...,t - 1\} $,

where $a(\tau)$ denotes the worker played at time $\tau$, ${r_{a(\tau )}}(\tau )$ denotes when chose arm $a$ the reward you have gotten. Given ${b_i}(t)$, the reward for arm i at time t is generated from an (unknown) distribution with mean ${b_i}(t)\mu$, where $\mu \in {R^d}$ is a fixed but unknown parameter \cite{agrawal2012thompson}.

$E[{r_i}(t)|\{ {b_i}(t)\} _{i = 1}^K,{H_{t - 1}}] = E[{r_i}(t)|\{ {b_i}(t)\} ] = {b_i}{(t)^T}\mu$.

At every time slot t, for the contextual bandit problem algorithm needs to select an arm $a(t)$ to play, using history $H _t-1$ and current contexts ${b _i}(t), i=1,...,K$. Let ${a^*}(t)$ denote the optimal arm at time t, i.e. ${a^*}(t) = \arg {\max _i}{b_i}{(t)^T}\mu $. And then the regret of strategy $\pi$ at time t is defined as follows:

\begin{equation}
{RE^\pi } = \sum\limits_{t = 1}^T {\{ {r^*}(t) - \sum\limits_{i = 1}^K {{a_i}{r_i}(t)} \} }
\end{equation}

The objective is to minimize the total regret $RE$ in overall T. The time horizon T is finite but possibly unknown.

Suppose that ${\eta _{i,t}} = {r_i}(t) - {b_i}{(t)^T}\mu $ is conditionally R-sub-Gaussian for a constant $c \ge 0$ just like in agrawal2012thompson, i.e.,

${\forall \lambda  \in R}$,$E[{e^{\lambda {\eta _{i,t}}}}|\{ {b_i}(t)\} _{i =1}^N,{H_{t - 1}}] \le \exp (\frac{{{\lambda ^2}{c^2}}}{2})$

This assumption is satisfied whenever ${r_i}(t) \in [{b_i}{(t)^T}\mu  - c,{b_i}{(t)^T}\mu  + c]$ (see Remark 1 in Appendix Filippi et al.(2010)). We will also assume that $\left\| {{b_i}(t)} \right\| \le 1,\left\| \mu  \right\| \le 1$, and $regre{t_i}(t) \le 1$ for all i,t (the norms, unless otherwise indicated, are $L _2$-norms). These assumptions are required to make the regret bounds scale-free, and are standard in the literature on this problem. If $\left\| {{b_i}(t)} \right\| \le \alpha ,\left\| \mu  \right\| \le \alpha , regret_i (t) \le \alpha$ instead, then our regret bounds would increase by a factor of $\alpha$.

we now define some of the desirable properties that an allocation policy show satisfy:\\
\emph{\textbf{Definition 1} Allocation Efficiency(AE)}: An allocation policy $\pi$ is said to be allocation efficiency if it minimize the regret. Formally, $\pi$ is allocation efficiency if:

\setlength{\parindent}{6em}$\pi  = \mathop {\arg \min R{E^\pi }}\limits_\pi$\\

\setlength{\parindent}{0em}\ebbmph{\textbf{Definition 2} Dominant Strategy Incentive Compatibility(DSIC):} Mechanism $M$ is said to be DSIC if truth telling is a dominant strategy for all the workers. Formally,


\emph{\textbf{Definition 3} Individual Rationality(IR):} Mechanism M is said to be IR for a worker if participating in mechanism always gives him positive utility. Formally,
%table
%\begin{tabular}{c|c}
%\toprule
%title&meaning\\
%\hline
%a&variable\\
%b&cdedfda\\
%c&kkkkkk\\
%\bottomrule
%\end{tabular}


\subsection{Problem}\setlength{\parindent}{2em}
Consider a mobile crowdsensing in the mobile social network. Without loss of generality, we let the requester be user ${v_0}$, and suppose that the requester has $m$ homogeneous mobile sensing tasks in one cycle, denoted by $j(n)$. The length of one cycle is $T$, and these tasks might be different types of task in different cycle. For simplicity, we assume that every requester will choose one worker from who ever get contacted with the requester for every task allocation and the requester's contact is built at the beginning within time $t$. It means if someone get contact with the requester during the period of the beginning $t$, the one will be add to the requester's contacts, regardless of who is the caller.

In this paper, we focus on how to choose a sequence of workers so as to maximize the utility of requester in every cycle.


\section{Task allocation algorithm}

\subsection {$\epsilon$-greedy-TA}
\newpage
\begin{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand\algorithmicensure {\textbf{Output:}}
\caption{$\varepsilon$-Greedy-TA}
%\label{$\epsilon$-greedy-TA}

\begin{algorithmic}[1]

\REQUIRE
    $K$: the number of arms;\newline
    $R$: reward function;\newline
    $T$: the round of times;\newline
    $\epsilon$: probability

\ENSURE  $r$:the accumulate reward.

\STATE $r=0;$

\STATE $\forall i = 1,2,...,K:Q(i) = 0,count(i) = 0;$
\FOR {$t=1,2,...,T $}

\IF {$rand()<\epsilon$}
\STATE randomly choose one from 1,2,...,K and assignment to k
\ELSE
\STATE $k = \mathop {\arg \max }\limits_i Q(i)$
\ENDIF
\STATE $v=R(k);$
\STATE $r=r+v;$
\STATE $Q(k) = \frac{{Q(k)*count(k) + v}}{{count(k) + 1}};$
\STATE $count(k)=count(k)+1;$

\ENDFOR
\end{algorithmic}
\end{algorithm}

%若摇臂奖励的不确定性较大，例如概率分布较宽时，则需要更多的探索，此时需要较大的\epsilon 值;
%若摇臂奖励的不确定性较小，例如概率分布较集中，则少量的尝试就能很好地接近真实奖励，此时需要较小的\epsilon。
if  the reward of the arm is very uncertainty, for example, its probability distrubutions contains too much value,
then it will need more exploration, and then it need big $\epsilon$ value.Vice versa.

\subsection {softmax-TA}

In softmax algorithm, it acccording to the known mean reward of the arms to decide to choose which arm.
If the mean reward is bigger, and the probability of choose this arm is bigger.

And the probability distribution of choose a arm is Boltzmann distributions:
\begin{equation}
P(k) = \frac{{{e^{\frac{{Q(k)}}{\tau }}}}}{{\sum\limits_{i = 1}^K {{e^{\frac{{Q(i)}}{\tau }}}} }}
\end{equation}

Q(i) represent the mean reward upon now; $\tau >0$ is called "temperature", and when $\tau$ is very small, the mean reward bigger one will have
higher probability to be choose.

\begin{algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand\algorithmicensure {\textbf{Output:}}
\caption{softmax-TA}
%\label{$\epsilon$-greedy-TA}

\begin{algorithmic}[2]

\REQUIRE
    $K$: the number of arms;\newline
    $R$: reward function;\newline
    $T$: the round of times;\newline
    $\tau$: temperature parameter

\ENSURE  $r$:the accumulate reward.

\STATE $r=0;$
\STATE $\forall i = 1,2,...,K:Q(i) = 0,count(i) = 0;$
\FOR {$t=1,2,...,T $}

\STATE randomly choose an arm from 1,2,...,K according to equation (1);
\STATE $v=R(k);$
\STATE $r=r+v;$
\STATE $Q(k) = \frac{{Q(k)*count(k) + v}}{{count(k) + 1}};$
\STATE $count(k)=count(k)+1;$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\subsection {Thompson Sampling for Task Allocation}
\setlength{\parindent}{2em}In this section, we will explain our Thompson Sampling algorithm for task allocation with incentive mechanism. The most important challenge is to balance exploitation with exploration. That is, we have two somewhat conflicting goals: (a) quickly find the best arm to pull and (b) pull the best arm as often as possible. Now we'll introduce the asymptotic optimal algorithm: Thompson Sampling.

Before I introduce the Thompson sampling, let's build up some intuition. If we have an infinite amount of pulls, then we know exactly what the expected rewards are, and there is no reason to ever explore. When we have a finite number of pulls, then we have to explore and the reason is that we are uncertain of what is the best arm. The right machinery to quantify uncertainty is the probability distribution. In Bayesian thinking, we want to use the entire probability distribution. Let's define ${p_a}(r)$ is the probability distribution from which rewards are drawn. That's what controls the bandit. We don't need to estimate the entire distribution for infinite amount of pulls, because all we care about is its mean ${\mu _a}$. What we will do is encode our uncertainty of ${\mu _a}$ in the probability distribution $p({\mu _a}|dat{a_a})$, and then use that probability distribution to decide when to explore and when to exploit.

let's say more clearly about the two probability:\\
 ${p_a}(r)$ - the probability distribution that bandit $a$ uses to generate rewards when it is pulled.\\
 $p({\mu _a}|dat{a_a})$ - the probability distribution of where we think the mean of ${p_a}(r)$ after observing some data.\\

 With Thompson sampling you keep around a probability distribution $p({\mu _a}|dat{a_a})$ that encodes your belief about where the expected reward ${\mu _a}$ is for arm $a$. For the simple coin-flip case, we can use the convenient Beta-Bernoulli model, and the distribution at round t after seeing ${S _a,t}$ successes and ${F _a,t}$ failures for arm $a$ is simply:\\
 $p({\mu _a}|dat{a_a}) = Beta({S _a,t}+1, {F_a,t}+1)$,\\
 where the added 1's are convenient priors.

 In our problem, we use Gaussian likelihood function and Gaussian prior to design our version of Thompson Sampling algorithm. More precisely, suppose that the likelihood of reward ${r _i}(t)$ at time t, given context ${b _i}(t)$ and parameter ${\mu}$, were given by the pdf of Gaussian distribution $N({b_i}{(t)^T}\mu ,{v ^2})$. Here, $v = R\sqrt {\frac{{24}}{\varepsilon }d\ln (\frac{1}{\delta })} $, with $\varepsilon  \in (0,1)$ which parameterizes our algorithm,just like \cite{agrawal2012thompson}. Let\\

 \setlength{\parindent}{4em}$B(t) = {I_d} + \sum\nolimits_{\tau  = 1}^{t - 1} {{b_{a(t)}}(\tau ){b_{a(t)}}{{(\tau )}^T}} $\\

 \setlength{\parindent}{4em}$\hat \mu (t) = B{(t)^{ - 1}}(\sum\nolimits_{\tau  = 1}^{t - 1} {{b_{a(t)}}(\tau ){r_{a(t)}}(\tau )} )$\\
 Then, if the prior for $\mu$ at time t is given by $N(\hat \mu (t),{v^2}B{(t)^{ - 1}})$, it is easy to compute the posterior distribution at time t+1,\\

 \setlength{\parindent}{4em} $\Pr (\hat \mu |{r_i}(t)) \propto \Pr ({r_i}(t)|\hat \mu )\Pr (\hat \mu)$\\
 as $N(\hat \mu (t),{v^2}B{(t)^{ - 1}})$ (detail of this computation are in Appendix A.1). In our Thompson Sampling algorithm, at every time step t, we will simply generate a sample ${\hat \mu(t)}$ from $N(\hat \mu (t),{v^2}B{(t)^{ - 1}})$, and play the arm $i$ that maximizes ${b_i}{(t)^T}\hat \mu (t)$.

\begin{algorithm}
\caption{ThompsonSampling-TA}
\begin{algorithmic}[3]

\STATE Set $B=I _d, \hat \mu=0_d, f = 0_d$
\FOR {$t=1,2,...,$}
\STATE Sample ${\hat \mu(t)}$ from distribution $N(\hat \mu (t),{v^2}B{(t)^{ - 1}})$.
\STATE Play arm $a(t):=argmax_i{b_i(t)^T\hat\mu(t)}$, and observe reward $r_t$.
\STATE Update $B=B+{b_{a(t)}}(t){b_{a(t)}}{(t)^T}, f=f+ {b_{a(t)}}(t)r_t, \hat \mu (t)=B^{-1} f$.
\IF {${r_t}=1$}
\STATE current arm is i(t), then we need to give him payback.
\STATE choose arm $j(t): = \mathop {\arg \max {\theta _j}(t)}\limits_{j \in K\backslash i}$ and observe reward ${r_t}^\prime $
\STATE Perform a Bernoulli trial with success probability ${{\tilde r}_t}^\prime$ and observe output ${r_t}^\prime$
\IF {${r_t}^\prime =1$}
\STATE ${p_i}^t = min({p_j}^t, p_max)$
\ELSE
\STATE ${p_i}^t=min({p_i}^t,p_max)$
\ENDIF
\ELSE
\STATE do not allocation this task at this round.
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

At every step t of Thompson Sampling task allocation algorithm consists of generating a d-dimensional sample ${\tilde \mu }$ from a multi-variate Gaussian distribution, and solving the problem $argmax_i{b_i(t)^T\hat\mu(t)}$. Therefore, even if the number of arms N is large (or infinite), the above algorithm is efficient as long as the problem $argmax_i{b_i(t)^T\hat\mu(t)}$ is efficiently solvable. After choose an arm, then, we should decide to give how much payment to that work. For this part, we have to make sure the worker is rational and truthful. For the payment, we first record the worker's bid who have the max ${b_i(t)^T\hat\mu(t)}$, and then we temporarily put the worker aside who has been selected just now, and selected the other worker who has the max ${b_i(t)^T\hat\mu(t)}$ over the rest of workers. The requester then collects the rest worker's bid and choose a property one's bid, which will be as the payment of the first worker. Just as in our algorithms.

\section{Analysis of AT-algorithm}

\section{Simulation Experiments}
\setlength{\parindent}{2em}This subsection gives a detail description of our experimental setup, including data collection, feature construction, performance evaluation.

\subsection{Data Collection}
Our dataset comes from MIT Reality Mining. It collected from 100 mobile phone users over the course of 9 month (September 2004 to June 2005) at MIT. The dataset includes the participants' message and phone call history, Bluetooth discovery records, answers of a survey etc. We again extract communication information and location based on phone call history. We use the before six month data as training set and the later part is used for testing algorithm. In train, we statistic every user who ever have gotten in touch with him, including call and message history. Through this, we can confirm everyone's contacts, and then in our algorithm we will regard the k members of his contacts as k arm bandits.

\subsection{Feature Construction}

We now describe the feature(also called context) constructed for our experiments. The features are used for our reward model, for we have define the reward as a linear payoffs.   
\subsection{Performance}

\section{Conclusion}

\setlength{\parindent}{2em}In this paper, we study the task allocation problem for crowdsensing. In order to select the best worker from the requester's contacts, we proposed thompson sampling allocation algorithm with incentive mechanism. This algorithm can make sure long run and keep the regret stay low. We showed that intuition from crowdsourcing mechansim is useful, and our algorithm follow the DISC and IR constrains. Analysis of TS-TA showed that it achieves regret of .




% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


\appendices
\section{Proof of the First Zonklar Equation}
Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
\section{}
Appendix two text goes here.


% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section*{Acknowledgment}
\fi


The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
%\ifCLASSOPTIONcaptionsoff
 % \newpage
%\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{IEEEhowto:kopka}
%H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

%\end{thebibliography}

%引用文献

\renewcommand\refname{REFERENCES}
\bibliographystyle{plain}
\bibliography{./myreference}


% biography section
%
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

%\begin{IEEEbiography}{Michael Shell}
%Biography text here.
%\end{IEEEbiography}

% if you will not have a photo at all:
%\begin{IEEEbiographynophoto}{John Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

%\begin{IEEEbiographynophoto}{Jane Doe}
%Biography text here.
%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}


% that's all folks
\end{document}


